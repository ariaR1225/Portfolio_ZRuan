<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.24.0 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>ChatGPT-powered Robot Chef - Zhian (Aria) Ruan</title>
<meta name="description" content="ChatGPT, CLIP, MediaPipe, LSTM, ROS2, MoveIt">


  <meta name="author" content="Zhian (Aria) Ruan">
  
  <meta property="article:author" content="Zhian (Aria) Ruan">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Zhian (Aria) Ruan">
<meta property="og:title" content="ChatGPT-powered Robot Chef">
<meta property="og:url" content="http://localhost:4000/portfolio_control_sw/winter-project/">


  <meta property="og:description" content="ChatGPT, CLIP, MediaPipe, LSTM, ROS2, MoveIt">



  <meta property="og:image" content="http://localhost:4000/assets/images/winter_gif.gif">





  <meta property="article:published_time" content="2024-06-05T22:31:16-07:00">






<link rel="canonical" href="http://localhost:4000/portfolio_control_sw/winter-project/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": null,
      "url": "http://localhost:4000/"
    
  }
</script>







<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Zhian (Aria) Ruan Feed">


<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single wide">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
          Zhian (Aria) Ruan
          
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/about/">More about me</a>
            </li><li class="masthead__menu-item">
              <a href="https://raw.githubusercontent.com/hang-yin/portfolio/gh-pages/assets/ZhianRuan_Robotics.pdf">Resume</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person" class="h-card">

  
    <div class="author__avatar">
      <a href="http://localhost:4000/">
        <img src="/assets/images/bio-photo1.png" alt="Zhian (Aria) Ruan" itemprop="image" class="u-photo">
      </a>
    </div>
  

  <div class="author__content">
    <h3 class="author__name p-name" itemprop="name">
      <a class="u-url" rel="me" href="http://localhost:4000/" itemprop="url">Zhian (Aria) Ruan</a>
    </h3>
    
      <div class="author__bio p-note" itemprop="description">
        <p>MS in Robotics @ Northwestern</p>

      </div>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      

      
        
          
            <li><a href="" rel="nofollow noopener noreferrer me"><i class="fas fa-fw fa-map-marker-alt" aria-hidden="true"></i><span class="label">Evanston, IL</span></a></li>
          
        
          
            <li><a href="https://github.com/hang-yin" rel="nofollow noopener noreferrer me" itemprop="sameAs"><i class="fab fa-fw fa-github" aria-hidden="true"></i><span class="label">GitHub</span></a></li>
          
        
          
            <li><a href="mailto:hangyin2023@u.northwestern.edu" rel="nofollow noopener noreferrer me"><i class="fa fa-fw fa-envelope" aria-hidden="true"></i><span class="label">Email</span></a></li>
          
        
          
            <li><a href="https://www.linkedin.com/in/hangyin0226/" rel="nofollow noopener noreferrer me" itemprop="sameAs"><i class="fab fa-linkedin" aria-hidden="true"></i><span class="label">LinkedIn</span></a></li>
          
        
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer me">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>



  <article class="page h-entry" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="ChatGPT-powered Robot Chef">
    <meta itemprop="description" content="ChatGPT, CLIP, MediaPipe, LSTM, ROS2, MoveIt">
    
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title p-name" itemprop="headline">
            <a href="http://localhost:4000/portfolio_control_sw/winter-project/" class="u-url" itemprop="url">ChatGPT-powered Robot Chef
</a>
          </h1>
          


        </header>
      

      <section class="page__content e-content" itemprop="text">
        
          <aside class="sidebar__right ">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#video-demo">Video Demo</a></li><li><a href="#system-architecture">System Architecture</a></li><li><a href="#alexa-custom-skill">Alexa Custom Skill</a></li><li><a href="#chatgpt-query">ChatGPT Query</a><ul><li><a href="#overview">Overview</a></li><li><a href="#comparison-with-other-gpt3-engines">Comparison with other GPT3 engines</a></li><li><a href="#chatgpt-wrapper">ChatGPT wrapper</a></li></ul></li><li><a href="#object-detection-with-clip">Object Detection with CLIP</a><ul><li><a href="#comparison-with-traditional-object-detection-methods">Comparison with traditional object detection methods</a></li><li><a href="#implementation-of-clip-for-multiple-objects">Implementation of CLIP for multiple objects</a></li></ul></li><li><a href="#hand-action-recognition-with-mediapipe--lstm">Hand Action Recognition with MediaPipe &amp; LSTM</a></li><li><a href="#motion-module">Motion Module</a></li><li><a href="#source-code">Source code</a></li><li><a href="#reference">Reference</a></li></ul>

            </nav>
          </aside>
        
        <p>This project is a system that enables voice-controlled, robot-assisted cooking. The system utilizes a custom Alexa skill to process user voice commands and a Flask app to generate recipe steps through ChatGPT. Object detection using a RealSense camera and the CLIP model allows the system to recognize objects in the kitchen and adjust recipe steps accordingly. The robot arm executes the steps autonomously, while a hand-action recognition model based on MediaPipe provides the necessary feedback to ensure collaboration between human and robot in completing the cooking tasks. This system contributes to home automation technology and showcases the integration of machine learning, computer vision, and robotics in real-world applications.</p>

<h2 id="video-demo">Video Demo</h2>
<iframe width="100%" height="50px" src="https://www.youtube.com/embed/k3ZFFC9DcEY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen="">
</iframe>

<h2 id="system-architecture">System Architecture</h2>

<p><img src="http://localhost:4000/assets/images/winter-system.png" alt="Project architecture" /></p>

<h2 id="alexa-custom-skill">Alexa Custom Skill</h2>

<p>To enable users to interact with this system more intuitively and conveniently, I built a custom Alexa skill that enables users to control the system through voice commands. The Alexa Developer console was used to build and deploy the custom skill on an Alexa dot, which converts audio to text and extracts the user’s intent, which refers to the recipe they want to cook. Once the intent is extracted, the Alexa custom skill sends an HTTP request to a local Flask app, which would then process the request with ChatGPT to generate recipe steps that the robot arm can execute.</p>

<h2 id="chatgpt-query">ChatGPT Query</h2>

<h3 id="overview">Overview</h3>

<p>To process the user’s intent and generate the recipe steps, the Flask app uses ChatGPT, a state-of-the-art language model that can process natural language input and generate text outputs. The Flask app builds up a prompt that includes example recipes, detected items using CLIP, and the current user intent/instruction. This prompt is then fed into ChatGPT, which generates a sequence of recipe steps that the robot arm can execute. By leveraging ChatGPT’s advanced language processing capabilities, the system can provide users with detailed, context-sensitive instructions that take into account the available ingredients, the user’s intent, and the current state of the kitchen. This allows the system to generate highly personalized and adaptive cooking instructions that are tailored to the specific needs of each user.</p>

<h3 id="comparison-with-other-gpt3-engines">Comparison with other GPT3 engines</h3>

<p>During the development of the system, I evaluated several language models, including GPT-3 engines from OpenAI such as Ada 001, Davinci 001, and Davinci 003. We implemented a comparison test that utilized a simulated pick and place example code from the open-source <a href="https://github.com/google-research/google-research/tree/master/saycan">SayCan project</a>. The test involved building a collection of options based on the items available in the kitchen and then feeding them into the different language models to obtain scores via text completion. The highest-scoring action was then selected and added to the current context, with the process repeating until the recipe was complete. While the GPT-3 engines worked well for simple recipes, they struggled to handle more complicated recipes, particularly those that required human action. After comparing the results of each engine, we found that ChatGPT outperformed the others, particularly on more complex recipes. A comparison chart between all the engines and the result from ChatGPT is included below, highlighting the superiority of ChatGPT in handling complex recipe instructions.</p>

<p><img src="http://localhost:4000/assets/images/engine-comparison.png" alt="Engine comparison" /></p>

<h3 id="chatgpt-wrapper">ChatGPT wrapper</h3>
<p>It is worth noting that at the time of this project, the ChatGPT API was not yet released by OpenAI. Therefore, a <a href="https://github.com/mmabrouk/chatgpt-wrapper">ChatGPT wrapper</a> was used to enable the integration of ChatGPT with the system. This wrapper allows the use of ChatGPT within a Python environment.</p>

<h2 id="object-detection-with-clip">Object Detection with CLIP</h2>

<p><img src="http://localhost:4000/assets/images/clip-result.png" alt="CLIP result" /></p>

<h3 id="comparison-with-traditional-object-detection-methods">Comparison with traditional object detection methods</h3>
<p>For object detection, the system uses a RealSense D435i camera and a machine learning model called CLIP. Unlike other state-of-the-art object detection models like DETR and YOLO v5, CLIP is trained on a massive dataset of 400 million (image, text) pairs, making it capable of recognizing out-of-vocabulary objects without the need for additional labeled data. Since CLIP can recognize objects based on their textual descriptions, it can also take into account different variations and synonyms of the same object, leading to a more robust detection performance. For example, we can ask CLIP to find a “red apple” or “chopped onion”.  The 3D location of objects in the kitchen is provided by CLIP, which produces center object bounding boxes and uses depth information from the RealSense camera. Initially, I experimented with other object detection models but found that they couldn’t recognize out-of-vocabulary objects without fine-tuning on every kitchen’s novel objects, which was not practical. In contrast, CLIP’s ability to recognize a wide range of objects without the need for additional labeling made it a more general and usable solution for the system.</p>

<h3 id="implementation-of-clip-for-multiple-objects">Implementation of CLIP for multiple objects</h3>

<p>While the original CLIP model only performs image classification for a single object, we need to perform object detection for multiple objects in our kitchen.</p>

<p>To utilize CLIP for object detection, I followed a tutorial called <a href="https://www.pinecone.io/learn/zero-shot-object-detection-clip/">Zero Shot Object Detection with OpenAI’s CLIP</a>. The basic idea behind this approach is to break an image into many small patches, and pass a window over these patches, generating an image embedding for each unique window. We can then calculate the similarity between these patch image embeddings and our class label embeddings, returning a score for each patch. After calculating the similarity scores for every patch, we collate them into a map of relevance across the entire image. We use that map to identify the location of the object of interest. Here’s an example for detecting an orange:</p>

<p><img src="http://localhost:4000/assets/images/clip-implementation.png" alt="CLIP implementation" /></p>

<p>Once we have identified the location of the object, we can recreate a bounding box around the object given a threshold for classification score. To achieve this, we used a RealSense D435i camera to obtain depth information, which we combined with the center of the object bounding box produced by CLIP to obtain 3D location information for the object.</p>

<h2 id="hand-action-recognition-with-mediapipe--lstm">Hand Action Recognition with MediaPipe &amp; LSTM</h2>

<p>For more complex recipes that require human collaboration, our system needs to be able to recognize when the user has completed their assigned task. To address this, I developed a hand action recognition module using MediaPipe and an LSTM neural network. Since it’s difficult for a single robot arm to perform tasks like cutting vegetables, we rely on the user to complete these tasks. However, we need a way to provide feedback to the system once the user has finished executing their tasks. To do this, I collected labeled video data and trained the LSTM neural network to classify video clips into hand actions such as grabbing and cutting behaviors. Instead of feeding entire video sequences into the neural network, I used MediaPipe to extract hand landmarks for faster processing and dimension reduction. Each video frame contained 21 hand landmarks with 3D coordinates. I collected 60 video clips for each hand action class, where each clip contained 45 frames. The LSTM was trained with categorical cross-entropy loss for 20 epochs and achieved 100% accuracy on the small testing set I collected. The architecture of the LSTM model is shown below:</p>

<p><img src="http://localhost:4000/assets/images/lstm-architecture.png" alt="LSTM architecture" /></p>

<p>The below video demonstrates the hand action recognition model in action.</p>

<iframe width="100%" height="50px" src="https://www.youtube.com/embed/L3d_Eli7Jmc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen="">
</iframe>

<h2 id="motion-module">Motion Module</h2>

<p>The motion module of the system is built on top of a custom Python MoveIt API that my team and I built for controlling the Franka Emika Panda robot arm in a <a href="https://hang-yin.github.io/portfolio/portfolio/jenga/">previous project</a>. The API provides an interface to control the robot arm with a simple Python script by specifying Cartesian positions. In this current project, I used the same API to control the robot arm to perform the various actions required in the recipe. For example, given the position of an ingredient, the API can move the end effector of the robot arm to that position and perform the appropriate action, such as picking up or placing the ingredient. The API also provides collision detection and avoidance capabilities, ensuring the robot arm does not collide with any other objects in the workspace. By building on top of this existing API, I was able to rapidly prototype the motion module of the system and focus on the integration with the other modules.</p>

<p><img src="http://localhost:4000/assets/images/motion-module.png" alt="motion-module" /></p>

<h2 id="source-code">Source code</h2>
<p><a href="https://github.com/hang-yin/Language-driven-Manipulation">Github repo</a></p>

<h2 id="reference">Reference</h2>
<ul>
  <li>Ahn, M., Brohan, A., Brown, N., Chebotar, Y., Cortes, O., David, B., Finn, C., Fu, C., Gopalakrishnan, K., Hausman, K., Herzog, A., Ho, D., Hsu, J., Ibarz, J., Ichter, B., Irpan, A., Jang, E., Ruano, R. J., Jeffrey, K., … Zeng, A. (2022). Do As I Can, Not As I Say: Grounding Language in Robotic Affordances. arXiv. <a href="https://doi.org/10.48550/ARXIV.2204.01691">https://doi.org/10.48550/ARXIV.2204.01691</a></li>
  <li>Lugaresi, C., Tang, J., Nash, H., McClanahan, C., Uboweja, E., Hays, M., Zhang, F., Chang, C.-L., Yong, M. G., Lee, J., Chang, W.-T., Hua, W., Georg, M., &amp; Grundmann, M. (2019). MediaPipe: A Framework for Building Perception Pipelines. arXiv. <a href="https://doi.org/10.48550/ARXIV.1906.08172">https://doi.org/10.48550/ARXIV.1906.08172</a></li>
  <li>Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., &amp; Sutskever, I. (2021). Learning Transferable Visual Models From Natural Language Supervision. arXiv. <a href="https://doi.org/10.48550/ARXIV.2103.00020">https://doi.org/10.48550/ARXIV.2103.00020</a></li>
</ul>

        
      </section>

      <footer class="page__meta">
        
        


        

      </footer>

      <section class="page__share">
  

  <a href="https://twitter.com/intent/tweet?text=ChatGPT-powered+Robot+Chef%20http%3A%2F%2Flocalhost%3A4000%2Fportfolio_control_sw%2Fwinter-project%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Flocalhost%3A4000%2Fportfolio_control_sw%2Fwinter-project%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=http%3A%2F%2Flocalhost%3A4000%2Fportfolio_control_sw%2Fwinter-project%2F" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/portfolio_control_sw/omnids/" class="pagination--pager" title="CVAE for Assistive Action Prediction
">Previous</a>
    
    
      <a href="#" class="pagination--pager disabled">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
</div>

    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
          <li><a href="https://github.com/hang-yin" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2024 Zhian (Aria) Ruan. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>







  </body>
</html>
